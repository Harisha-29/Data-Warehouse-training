{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07f6dec9-121c-4c72-ab90-e23d3d27ed1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"Set3_EmployeeProject\").getOrCreate()\n",
    "\n",
    "# Employee Data\n",
    "employee_data = [\n",
    "    (\"Ananya\", \"HR\", 52000),\n",
    "    (\"Rahul\", \"Engineering\", 65000),\n",
    "    (\"Priya\", \"Engineering\", 60000),\n",
    "    (\"Zoya\", \"Marketing\", 48000),\n",
    "    (\"Karan\", \"HR\", 53000),\n",
    "    (\"Naveen\", \"Engineering\", 70000),\n",
    "    (\"Fatima\", \"Marketing\", 45000)\n",
    "]\n",
    "columns_emp = [\"Name\", \"Department\", \"Salary\"]\n",
    "df_emp = spark.createDataFrame(employee_data, columns_emp)\n",
    "\n",
    "# Performance Data\n",
    "performance_data = [\n",
    "    (\"Ananya\", 2023, 4.5),\n",
    "    (\"Rahul\", 2023, 4.9),\n",
    "    (\"Priya\", 2023, 4.3),\n",
    "    (\"Zoya\", 2023, 3.8),\n",
    "    (\"Karan\", 2023, 4.1),\n",
    "    (\"Naveen\", 2023, 4.7),\n",
    "    (\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance_data, columns_perf)\n",
    "\n",
    "# Project Data\n",
    "project_data = [\n",
    "    (\"Ananya\", \"HR Portal\", 120),\n",
    "    (\"Rahul\", \"Data Platform\", 200),\n",
    "    (\"Priya\", \"Data Platform\", 180),\n",
    "    (\"Zoya\", \"Campaign Tracker\", 100),\n",
    "    (\"Karan\", \"HR Portal\", 130),\n",
    "    (\"Naveen\", \"ML Pipeline\", 220),\n",
    "    (\"Fatima\", \"Campaign Tracker\", 90)\n",
    "]\n",
    "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
    "df_proj = spark.createDataFrame(project_data, columns_proj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf8b6c5a-7e3c-46f8-ad4c-b015a6ece709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n+------+-----------+------+----+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_joined = df_emp \\\n",
    "    .join(df_perf, on=\"Name\", how=\"inner\") \\\n",
    "    .join(df_proj, on=\"Name\", how=\"inner\")\n",
    "\n",
    "df_joined.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c287d02-e201-43d5-bcd8-932042fa53b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n| Department|TotalHours|\n+-----------+----------+\n|         HR|       250|\n|Engineering|       600|\n|  Marketing|       190|\n+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_joined.groupBy(\"Department\").agg(sum(\"HoursWorked\").alias(\"TotalHours\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1c9778-3cbf-4b85-a54c-b2e4a2083d4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n|         Project|         AvgRating|\n+----------------+------------------+\n|       HR Portal|               4.3|\n|   Data Platform|               4.6|\n|Campaign Tracker|3.8499999999999996|\n|     ML Pipeline|               4.7|\n+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_joined.groupBy(\"Project\").agg(avg(\"Rating\").alias(\"AvgRating\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6844c388-eba7-43a1-a3cf-67caa5d6ef1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n|  Temp|2023|  NULL|\n+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define schema matching df_perf\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Rating\", DoubleType(), True)  # Explicitly say it's DoubleType\n",
    "])\n",
    "\n",
    "# Create DataFrame with a row that has a null Rating\n",
    "extra_row = spark.createDataFrame([(\"Temp\", 2023, None)], schema=schema)\n",
    "\n",
    "# Union with original DataFrame\n",
    "df_perf_null = df_perf.unionByName(extra_row)\n",
    "df_perf_null.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c387de-60a8-4da5-ac45-2574dc9a2063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n|Name|Year|Rating|\n+----+----+------+\n|Temp|2023|  NULL|\n+----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_perf_null.filter(col(\"Rating\").isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c9aa58a-8cec-4023-ab75-504e5c71c15a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+------------+\n|  Name| Department|Rating|RatingFilled|\n+------+-----------+------+------------+\n|Ananya|         HR|   4.5|         4.5|\n| Rahul|Engineering|   4.9|         4.9|\n| Priya|Engineering|   4.3|         4.3|\n|  Zoya|  Marketing|   3.8|         3.8|\n| Karan|         HR|   4.1|         4.1|\n|Naveen|Engineering|   4.7|         4.7|\n|Fatima|  Marketing|   3.9|         3.9|\n|  Temp|       NULL|  NULL|        NULL|\n+------+-----------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Join to get department for each employee\n",
    "df_perf_with_dept = df_perf_null.join(df_emp, on=\"Name\", how=\"left\")\n",
    "\n",
    "# Calculate avg rating per department (excluding nulls)\n",
    "avg_rating_per_dept = df_perf_with_dept.groupBy(\"Department\") \\\n",
    "    .agg(avg(\"Rating\").alias(\"DeptAvgRating\"))\n",
    "\n",
    "# Join back to get dept average for each row\n",
    "df_with_avg = df_perf_with_dept.join(avg_rating_per_dept, on=\"Department\", how=\"left\")\n",
    "\n",
    "# Fill null ratings\n",
    "df_filled = df_with_avg.withColumn(\n",
    "    \"RatingFilled\", when(col(\"Rating\").isNull(), col(\"DeptAvgRating\")).otherwise(col(\"Rating\"))\n",
    ")\n",
    "df_filled.select(\"Name\", \"Department\", \"Rating\", \"RatingFilled\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfff53c6-fee8-4b8b-b98b-51d015d32235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------------+\n|  Name|Rating|PerformanceCategory|\n+------+------+-------------------+\n|Ananya|   4.5|               Good|\n| Priya|   4.3|               Good|\n| Rahul|   4.9|          Excellent|\n|  Zoya|   3.8|            Average|\n| Karan|   4.1|               Good|\n|Naveen|   4.7|          Excellent|\n|Fatima|   3.9|            Average|\n+------+------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_perf_cat = df_joined.withColumn(\n",
    "    \"PerformanceCategory\",\n",
    "    when(col(\"Rating\") >= 4.7, \"Excellent\")\n",
    "    .when((col(\"Rating\") >= 4.0) & (col(\"Rating\") < 4.7), \"Good\")\n",
    "    .otherwise(\"Average\")\n",
    ")\n",
    "df_with_perf_cat.select(\"Name\", \"Rating\", \"PerformanceCategory\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3425cc7-e5a1-44d7-8221-de7cf10ce952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----+\n|  Name|HoursWorked|Bonus|\n+------+-----------+-----+\n|Ananya|        120| 5000|\n| Rahul|        200| 5000|\n| Priya|        180| 5000|\n|  Zoya|        100| 5000|\n| Karan|        130| 5000|\n|Naveen|        220|10000|\n|Fatima|         90| 5000|\n+------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def assign_bonus(hours):\n",
    "    return 10000 if hours > 200 else 5000\n",
    "\n",
    "bonus_udf = udf(assign_bonus, IntegerType())\n",
    "\n",
    "df_with_bonus = df_with_perf_cat.withColumn(\"Bonus\", bonus_udf(col(\"HoursWorked\")))\n",
    "df_with_bonus.select(\"Name\", \"HoursWorked\", \"Bonus\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e77d206-765e-4285-a69b-a97bc1202aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+\n|  Name|  JoinDate|MonthsWorked|\n+------+----------+------------+\n|Ananya|2021-06-01|          49|\n| Priya|2021-06-01|          49|\n| Rahul|2021-06-01|          49|\n|  Zoya|2021-06-01|          49|\n| Karan|2021-06-01|          49|\n|Naveen|2021-06-01|          49|\n|Fatima|2021-06-01|          49|\n+------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_dates = df_with_bonus.withColumn(\"JoinDate\", lit(\"2021-06-01\").cast(\"date\"))\n",
    "df_with_dates = df_with_dates.withColumn(\n",
    "    \"MonthsWorked\",\n",
    "    floor(datediff(current_date(), col(\"JoinDate\")) / 30)\n",
    ")\n",
    "df_with_dates.select(\"Name\", \"JoinDate\", \"MonthsWorked\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9cf4423-3baf-4a23-8431-d20ead6f20e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|  Name|  JoinDate|\n+------+----------+\n|Ananya|2021-06-01|\n| Priya|2021-06-01|\n| Rahul|2021-06-01|\n|  Zoya|2021-06-01|\n| Karan|2021-06-01|\n|Naveen|2021-06-01|\n|Fatima|2021-06-01|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_dates.filter(col(\"JoinDate\") < lit(\"2022-01-01\")).select(\"Name\", \"JoinDate\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f237e6-fd3f-43e3-96ca-5ac47c6a87a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n| Meena|         HR| 48000|\n|   Raj|  Marketing| 51000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "extra_employees = [\n",
    "    (\"Meena\", \"HR\", 48000),\n",
    "    (\"Raj\", \"Marketing\", 51000)\n",
    "]\n",
    "df_extra = spark.createDataFrame(extra_employees, columns_emp)\n",
    "\n",
    "df_union = df_emp.unionByName(df_extra)\n",
    "df_union.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343bc58f-359c-46a0-9fc6-9f018ebf239d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+------+----------------+-----------+-------------------+-----+----------+------------+-----------+\n|  Name|Salary|Year|Rating|         Project|HoursWorked|PerformanceCategory|Bonus|  JoinDate|MonthsWorked| Department|\n+------+------+----+------+----------------+-----------+-------------------+-----+----------+------------+-----------+\n| Rahul| 65000|2023|   4.9|   Data Platform|        200|          Excellent| 5000|2021-06-01|          49|Engineering|\n| Priya| 60000|2023|   4.3|   Data Platform|        180|               Good| 5000|2021-06-01|          49|Engineering|\n|Fatima| 45000|2023|   3.9|Campaign Tracker|         90|            Average| 5000|2021-06-01|          49|  Marketing|\n|  Zoya| 48000|2023|   3.8|Campaign Tracker|        100|            Average| 5000|2021-06-01|          49|  Marketing|\n|Naveen| 70000|2023|   4.7|     ML Pipeline|        220|          Excellent|10000|2021-06-01|          49|Engineering|\n|Ananya| 52000|2023|   4.5|       HR Portal|        120|               Good| 5000|2021-06-01|          49|         HR|\n| Karan| 53000|2023|   4.1|       HR Portal|        130|               Good| 5000|2021-06-01|          49|         HR|\n+------+------+----+------+----------------+-----------+-------------------+-----+----------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_read_back = spark.read.parquet(\"/path/to/output/final_employee_data_partitioned\")\n",
    "df_read_back.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-06-11 14:59:31",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}